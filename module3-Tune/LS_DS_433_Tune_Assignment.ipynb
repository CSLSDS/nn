{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_433_Tune_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "U4S2NN (Py3.7)",
      "language": "python",
      "name": "nn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0-final"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Train Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Using your baseline model from yesterday, hyperparameter tune it and report on your highest validation accuracy. **Your singular goal today is to achieve the highest accuracy possible.**\n",
        "\n",
        "*Don't forgot to switch to GPU on Colab!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptJ2b3wk62Ud",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters to Tune\n",
        "\n",
        "At a minimum, tune each of these hyperparameters using any strategy we discussed during lecture today: \n",
        "- Optimizer\n",
        "- Learning Rate\n",
        "- Activiation Function\n",
        "  - At least 1 subparameter within the Relu activation function\n",
        "- Number of Neurons in Hidden Layers\n",
        "- Number of Hidden Layers\n",
        "- Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "\n",
        "data = np.load('quickdraw10.npz')\n",
        "X = data['arr_0']\n",
        "y = data['arr_1']\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(100000, 784)\n(100000,)\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Need this line to randomly shuffle both the X & y at the same time.\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "X, y = shuffle(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "((80000, 784), (80000,), (20000, 784), (20000,))"
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning batch size..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "00 [==============================] - 3s 3ms/step - loss: 0.8928 - accuracy: 0.7284\nEpoch 16/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 0.8847 - accuracy: 0.7316\nEpoch 17/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 0.8887 - accuracy: 0.7305\nEpoch 18/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 0.8824 - accuracy: 0.7319\nEpoch 19/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 0.8785 - accuracy: 0.7319\nEpoch 20/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 0.8775 - accuracy: 0.7340\n250/250 [==============================] - 1s 2ms/step - loss: 0.9441 - accuracy: 0.7189\nEpoch 1/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 3.6213 - accuracy: 0.2846\nEpoch 2/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 1.5979 - accuracy: 0.4442\nEpoch 3/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.3580 - accuracy: 0.5016\nEpoch 4/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 1.2483 - accuracy: 0.5367\nEpoch 5/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 1.1906 - accuracy: 0.5550\nEpoch 6/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.1432 - accuracy: 0.5787\nEpoch 7/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0969 - accuracy: 0.6120\nEpoch 8/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 1.0829 - accuracy: 0.6267\nEpoch 9/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0684 - accuracy: 0.6315\nEpoch 10/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0646 - accuracy: 0.6332\nEpoch 11/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 1.0640 - accuracy: 0.6337\nEpoch 12/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0548 - accuracy: 0.6363\nEpoch 13/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0526 - accuracy: 0.6390\nEpoch 14/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 1.0455 - accuracy: 0.6407\nEpoch 15/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0491 - accuracy: 0.6376\nEpoch 16/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0436 - accuracy: 0.6404\nEpoch 17/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0441 - accuracy: 0.6418\nEpoch 18/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 1.0401 - accuracy: 0.6426\nEpoch 19/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0378 - accuracy: 0.6423\nEpoch 20/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0364 - accuracy: 0.6412\n250/250 [==============================] - 1s 3ms/step - loss: 1.1342 - accuracy: 0.6359\nEpoch 1/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 3.1135 - accuracy: 0.3025\nEpoch 2/20\n1000/1000 [==============================] - 3s 3ms/step - loss: 1.6738 - accuracy: 0.3737\nEpoch 3/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.5602 - accuracy: 0.4106\nEpoch 4/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 1.4779 - accuracy: 0.4384\nEpoch 5/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 1.3693 - accuracy: 0.4819\nEpoch 6/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.2800 - accuracy: 0.5272\nEpoch 7/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.2003 - accuracy: 0.5670\nEpoch 8/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0953 - accuracy: 0.6219\nEpoch 9/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0242 - accuracy: 0.6723\nEpoch 10/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 0.9916 - accuracy: 0.6906\nEpoch 11/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 0.9736 - accuracy: 0.6991\nEpoch 12/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 0.9593 - accuracy: 0.7041\nEpoch 13/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9468 - accuracy: 0.7114\nEpoch 14/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9374 - accuracy: 0.7155\nEpoch 15/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 0.9235 - accuracy: 0.7207\nEpoch 16/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9176 - accuracy: 0.7219\nEpoch 17/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9113 - accuracy: 0.7232\nEpoch 18/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 0.9077 - accuracy: 0.7247\nEpoch 19/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9030 - accuracy: 0.7260\nEpoch 20/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9010 - accuracy: 0.7260\n250/250 [==============================] - 1s 2ms/step - loss: 0.9685 - accuracy: 0.7113\nEpoch 1/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 3.1798 - accuracy: 0.2064\nEpoch 2/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.7115 - accuracy: 0.3898\nEpoch 3/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.5308 - accuracy: 0.4542\nEpoch 4/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.3683 - accuracy: 0.5075\nEpoch 5/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 1.2470 - accuracy: 0.5586\nEpoch 6/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 1.1629 - accuracy: 0.5901\nEpoch 7/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0895 - accuracy: 0.6313\nEpoch 8/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 1.0462 - accuracy: 0.6473\nEpoch 9/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0167 - accuracy: 0.6550\nEpoch 10/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 1.0032 - accuracy: 0.6620\nEpoch 11/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9888 - accuracy: 0.6677\nEpoch 12/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9748 - accuracy: 0.6717\nEpoch 13/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9655 - accuracy: 0.6750\nEpoch 14/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9576 - accuracy: 0.6753\nEpoch 15/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9502 - accuracy: 0.6812\nEpoch 16/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9472 - accuracy: 0.6823\nEpoch 17/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9473 - accuracy: 0.6835\nEpoch 18/20\n1000/1000 [==============================] - 4s 4ms/step - loss: 0.9411 - accuracy: 0.6842\nEpoch 19/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 0.9414 - accuracy: 0.6862\nEpoch 20/20\n1000/1000 [==============================] - 5s 5ms/step - loss: 0.9387 - accuracy: 0.6854\n250/250 [==============================] - 1s 3ms/step - loss: 0.9946 - accuracy: 0.6848\nEpoch 1/20\n125/125 [==============================] - 1s 8ms/step - loss: 10.8660 - accuracy: 0.1796\nEpoch 2/20\n125/125 [==============================] - 1s 8ms/step - loss: 2.1631 - accuracy: 0.2244\nEpoch 3/20\n125/125 [==============================] - 1s 7ms/step - loss: 2.0184 - accuracy: 0.2822\nEpoch 4/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.8980 - accuracy: 0.3258\nEpoch 5/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.8060 - accuracy: 0.3533\nEpoch 6/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.7129 - accuracy: 0.3919\nEpoch 7/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.6090 - accuracy: 0.4358\nEpoch 8/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.5294 - accuracy: 0.4638\nEpoch 9/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.4600 - accuracy: 0.4848\nEpoch 10/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.4048 - accuracy: 0.4997\nEpoch 11/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.3547 - accuracy: 0.5135\nEpoch 12/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.3139 - accuracy: 0.5247\nEpoch 13/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2875 - accuracy: 0.5356\nEpoch 14/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2556 - accuracy: 0.5458\nEpoch 15/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2375 - accuracy: 0.5507\nEpoch 16/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2148 - accuracy: 0.5580\nEpoch 17/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.1968 - accuracy: 0.5642\nEpoch 18/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.1843 - accuracy: 0.5702\nEpoch 19/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.1658 - accuracy: 0.5774\nEpoch 20/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.1477 - accuracy: 0.5862\n32/32 [==============================] - 0s 4ms/step - loss: 1.2894 - accuracy: 0.5784\nEpoch 1/20\n125/125 [==============================] - 1s 8ms/step - loss: 7.7790 - accuracy: 0.2382\nEpoch 2/20\n125/125 [==============================] - 1s 8ms/step - loss: 2.0062 - accuracy: 0.2668\nEpoch 3/20\n125/125 [==============================] - 1s 10ms/step - loss: 1.9241 - accuracy: 0.2863\nEpoch 4/20\n125/125 [==============================] - 1s 9ms/step - loss: 1.8614 - accuracy: 0.3110\nEpoch 5/20\n125/125 [==============================] - 1s 9ms/step - loss: 1.8044 - accuracy: 0.3338\nEpoch 6/20\n125/125 [==============================] - 1s 9ms/step - loss: 1.7360 - accuracy: 0.3666\nEpoch 7/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.6593 - accuracy: 0.3969\nEpoch 8/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.5941 - accuracy: 0.4188\nEpoch 9/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.5345 - accuracy: 0.4380\nEpoch 10/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.4654 - accuracy: 0.4658\nEpoch 11/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.4043 - accuracy: 0.4838\nEpoch 12/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.3596 - accuracy: 0.4940\nEpoch 13/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.3319 - accuracy: 0.5020\nEpoch 14/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.3140 - accuracy: 0.5089\nEpoch 15/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2920 - accuracy: 0.5164\nEpoch 16/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2738 - accuracy: 0.5252\nEpoch 17/20\n125/125 [==============================] - 1s 6ms/step - loss: 1.2508 - accuracy: 0.5362\nEpoch 18/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2358 - accuracy: 0.5398\nEpoch 19/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2153 - accuracy: 0.5548\nEpoch 20/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.2012 - accuracy: 0.5616\n32/32 [==============================] - 0s 4ms/step - loss: 1.2827 - accuracy: 0.5453\nEpoch 1/20\n125/125 [==============================] - 1s 8ms/step - loss: 9.1390 - accuracy: 0.1829\nEpoch 2/20\n125/125 [==============================] - 1s 7ms/step - loss: 2.0923 - accuracy: 0.2258\nEpoch 3/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.9476 - accuracy: 0.2928\nEpoch 4/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.8474 - accuracy: 0.3226\nEpoch 5/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.7708 - accuracy: 0.3451\nEpoch 6/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.7128 - accuracy: 0.3605\nEpoch 7/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.6695 - accuracy: 0.3722\nEpoch 8/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.6249 - accuracy: 0.3880\nEpoch 9/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.5951 - accuracy: 0.3996\nEpoch 10/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.5620 - accuracy: 0.4101\nEpoch 11/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.5249 - accuracy: 0.4210\nEpoch 12/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.4922 - accuracy: 0.4335\nEpoch 13/20\n125/125 [==============================] - 1s 11ms/step - loss: 1.4569 - accuracy: 0.4529\nEpoch 14/20\n125/125 [==============================] - 1s 10ms/step - loss: 1.4133 - accuracy: 0.4684\nEpoch 15/20\n125/125 [==============================] - 1s 9ms/step - loss: 1.3721 - accuracy: 0.4913\nEpoch 16/20\n125/125 [==============================] - 1s 9ms/step - loss: 1.3235 - accuracy: 0.5172\nEpoch 17/20\n125/125 [==============================] - 1s 6ms/step - loss: 1.2754 - accuracy: 0.5367\nEpoch 18/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.2338 - accuracy: 0.5540\nEpoch 19/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.2049 - accuracy: 0.5698\nEpoch 20/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.1761 - accuracy: 0.5831\n32/32 [==============================] - 0s 4ms/step - loss: 1.3117 - accuracy: 0.5821\nEpoch 1/20\n125/125 [==============================] - 1s 7ms/step - loss: 8.6925 - accuracy: 0.1818\nEpoch 2/20\n125/125 [==============================] - 1s 7ms/step - loss: 2.1095 - accuracy: 0.2361\nEpoch 3/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.8952 - accuracy: 0.3338\nEpoch 4/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.7412 - accuracy: 0.3837\nEpoch 5/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.6326 - accuracy: 0.4181\nEpoch 6/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.5481 - accuracy: 0.4433\nEpoch 7/20\n125/125 [==============================] - 1s 9ms/step - loss: 1.4841 - accuracy: 0.4660\nEpoch 8/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.4207 - accuracy: 0.4947\nEpoch 9/20\n125/125 [==============================] - 1s 9ms/step - loss: 1.3678 - accuracy: 0.5119\nEpoch 10/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.3220 - accuracy: 0.5291\nEpoch 11/20\n125/125 [==============================] - 1s 9ms/step - loss: 1.2741 - accuracy: 0.5499\nEpoch 12/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.2329 - accuracy: 0.5685\nEpoch 13/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.1961 - accuracy: 0.5864\nEpoch 14/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.1671 - accuracy: 0.5970\nEpoch 15/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.1433 - accuracy: 0.6054\nEpoch 16/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.1180 - accuracy: 0.6169\nEpoch 17/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.0993 - accuracy: 0.6188\nEpoch 18/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.0868 - accuracy: 0.6232\nEpoch 19/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.0661 - accuracy: 0.6288\nEpoch 20/20\n125/125 [==============================] - 1s 10ms/step - loss: 1.0521 - accuracy: 0.6348\n32/32 [==============================] - 0s 6ms/step - loss: 1.1905 - accuracy: 0.6403\nEpoch 1/20\n125/125 [==============================] - 1s 9ms/step - loss: 10.3421 - accuracy: 0.2129\nEpoch 2/20\n125/125 [==============================] - 1s 10ms/step - loss: 2.0871 - accuracy: 0.2397\nEpoch 3/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.9670 - accuracy: 0.2887\nEpoch 4/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.8603 - accuracy: 0.3315\nEpoch 5/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.7574 - accuracy: 0.3697\nEpoch 6/20\n125/125 [==============================] - 1s 8ms/step - loss: 1.6615 - accuracy: 0.4083\nEpoch 7/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.5623 - accuracy: 0.4448\nEpoch 8/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.4672 - accuracy: 0.4823\nEpoch 9/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.3951 - accuracy: 0.5053\nEpoch 10/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.3410 - accuracy: 0.5211\nEpoch 11/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2883 - accuracy: 0.5397\nEpoch 12/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2468 - accuracy: 0.5523\nEpoch 13/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.2115 - accuracy: 0.5660\nEpoch 14/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.1800 - accuracy: 0.5782\nEpoch 15/20\n125/125 [==============================] - 1s 6ms/step - loss: 1.1572 - accuracy: 0.5838\nEpoch 16/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.1275 - accuracy: 0.5938\nEpoch 17/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.1043 - accuracy: 0.6026\nEpoch 18/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.0822 - accuracy: 0.6120\nEpoch 19/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.0603 - accuracy: 0.6190\nEpoch 20/20\n125/125 [==============================] - 1s 7ms/step - loss: 1.0466 - accuracy: 0.6248\n32/32 [==============================] - 0s 5ms/step - loss: 1.2274 - accuracy: 0.6125\nEpoch 1/20\n1250/1250 [==============================] - 4s 4ms/step - loss: 2.8117 - accuracy: 0.3374\nEpoch 2/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 1.4970 - accuracy: 0.4869\nEpoch 3/20\n1250/1250 [==============================] - 6s 5ms/step - loss: 1.2596 - accuracy: 0.5708\nEpoch 4/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 1.1243 - accuracy: 0.6207\nEpoch 5/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 1.0445 - accuracy: 0.6613\nEpoch 6/20\n1250/1250 [==============================] - 6s 4ms/step - loss: 0.9928 - accuracy: 0.6924\nEpoch 7/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 0.9415 - accuracy: 0.7129\nEpoch 8/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 0.9053 - accuracy: 0.7260\nEpoch 9/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 0.8962 - accuracy: 0.7295\nEpoch 10/20\n1250/1250 [==============================] - 4s 4ms/step - loss: 0.8795 - accuracy: 0.7332\nEpoch 11/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 0.8667 - accuracy: 0.7431\nEpoch 12/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 0.8455 - accuracy: 0.7505\nEpoch 13/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 0.8364 - accuracy: 0.7528\nEpoch 14/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 0.8316 - accuracy: 0.7544\nEpoch 15/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 0.8259 - accuracy: 0.7567\nEpoch 16/20\n1250/1250 [==============================] - 4s 4ms/step - loss: 0.8239 - accuracy: 0.7559\nEpoch 17/20\n1250/1250 [==============================] - 4s 3ms/step - loss: 0.8196 - accuracy: 0.7590\nEpoch 18/20\n1250/1250 [==============================] - 5s 4ms/step - loss: 0.8201 - accuracy: 0.7579\nEpoch 19/20\n1250/1250 [==============================] - 6s 5ms/step - loss: 0.8122 - accuracy: 0.7615\nEpoch 20/20\n1250/1250 [==============================] - 6s 5ms/step - loss: 0.8115 - accuracy: 0.7607\nBest: 0.6964499950408936 using {'batch_size': 64, 'epochs': 20}\nMeans: 0.6869124889373779, Stdev: 0.03342348148356944 with: {'batch_size': 32, 'epochs': 20}\nMeans: 0.6964499950408936, Stdev: 0.03391919045603087 with: {'batch_size': 64, 'epochs': 20}\nMeans: 0.5917125105857849, Stdev: 0.03228243911004746 with: {'batch_size': 512, 'epochs': 20}\n"
        }
      ],
      "source": [
        "# lecture method - tuning batch size\n",
        "import numpy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(units=32):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units, input_dim=784, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'batch_size': [32, 64, 512],\n",
        "              'epochs': [20]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid, \n",
        "    n_jobs=1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning number of layers..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "layers = Sequential()\n",
        "layers.add(Dense(64, input_dim=784, activation='relu'))\n",
        "layers.add(Dense(64, activation='selu'))\n",
        "layers.add(Dense(64, activation='selu'))\n",
        "layers.add(Dense(64, activation='selu'))\n",
        "layers.add(Dense(64, activation='selu'))\n",
        "layers.add(Dense(10, activation='softmax'))\n",
        "    # Compile model\n",
        "layers.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n2500/2500 [==============================] - 54s 22ms/step - loss: 1.1872 - accuracy: 0.6875 - val_loss: 0.7051 - val_accuracy: 0.7867\nEpoch 2/5\n2500/2500 [==============================] - 54s 22ms/step - loss: 0.6535 - accuracy: 0.8011 - val_loss: 0.6265 - val_accuracy: 0.8141\nEpoch 3/5\n2500/2500 [==============================] - 53s 21ms/step - loss: 0.5838 - accuracy: 0.8242 - val_loss: 0.6121 - val_accuracy: 0.8192\nEpoch 4/5\n2500/2500 [==============================] - 46s 18ms/step - loss: 0.5359 - accuracy: 0.8380 - val_loss: 0.5939 - val_accuracy: 0.8235\nEpoch 5/5\n2500/2500 [==============================] - 53s 21ms/step - loss: 0.5009 - accuracy: 0.8495 - val_loss: 0.5570 - val_accuracy: 0.8338\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f419845be10>"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "layers.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test)) # 6 layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tenlayers = Sequential()\n",
        "tenlayers.add(Dense(64, input_dim=784, activation='relu'))\n",
        "tenlayers.add(Dense(64, activation='selu'))\n",
        "tenlayers.add(Dense(64, activation='selu'))\n",
        "tenlayers.add(Dense(64, activation='selu'))\n",
        "tenlayers.add(Dense(64, activation='selu'))\n",
        "tenlayers.add(Dense(64, activation='selu'))\n",
        "tenlayers.add(Dense(64, activation='selu'))\n",
        "tenlayers.add(Dense(64, activation='selu'))\n",
        "tenlayers.add(Dense(64, activation='selu'))\n",
        "tenlayers.add(Dense(10, activation='softmax'))\n",
        "    # Compile model\n",
        "tenlayers.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n2500/2500 [==============================] - 14s 6ms/step - loss: 0.8842 - accuracy: 0.7327 - val_loss: 0.6699 - val_accuracy: 0.7951\nEpoch 2/5\n2500/2500 [==============================] - 12s 5ms/step - loss: 0.6145 - accuracy: 0.8153 - val_loss: 0.5903 - val_accuracy: 0.8248\nEpoch 3/5\n2500/2500 [==============================] - 12s 5ms/step - loss: 0.5517 - accuracy: 0.8345 - val_loss: 0.5623 - val_accuracy: 0.8326\nEpoch 4/5\n2500/2500 [==============================] - 13s 5ms/step - loss: 0.5105 - accuracy: 0.8465 - val_loss: 0.5614 - val_accuracy: 0.8285\nEpoch 5/5\n2500/2500 [==============================] - 13s 5ms/step - loss: 0.4784 - accuracy: 0.8562 - val_loss: 0.5469 - val_accuracy: 0.8368\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f41931b62b0>"
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tenlayers.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test)) # 6 layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ...seems like more layers is generally better, but marginal returns difference between 6 and 10 layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning number of nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n2500/2500 [==============================] - 54s 22ms/step - loss: 0.9917 - accuracy: 0.7683 - val_loss: 0.6059 - val_accuracy: 0.8226\nEpoch 2/5\n2500/2500 [==============================] - 49s 20ms/step - loss: 0.5407 - accuracy: 0.8367 - val_loss: 0.5349 - val_accuracy: 0.8399\nEpoch 3/5\n2500/2500 [==============================] - 51s 20ms/step - loss: 0.4810 - accuracy: 0.8576 - val_loss: 0.5205 - val_accuracy: 0.8469\nEpoch 4/5\n2500/2500 [==============================] - 49s 20ms/step - loss: 0.4284 - accuracy: 0.8736 - val_loss: 0.5012 - val_accuracy: 0.8508\nEpoch 5/5\n2500/2500 [==============================] - 42s 17ms/step - loss: 0.3828 - accuracy: 0.8877 - val_loss: 0.5041 - val_accuracy: 0.8571\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f4191fc2fd0>"
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\n",
        "# trying a more nuanced decrementing of nodes\n",
        "nodes = Sequential()\n",
        "nodes.add(Dense(784, input_dim=784, activation='relu'))\n",
        "nodes.add(Dense(512, activation='selu'))\n",
        "nodes.add(Dense(256, activation='selu'))\n",
        "nodes.add(Dense(128, activation='selu'))\n",
        "nodes.add(Dense(64, activation='selu'))\n",
        "nodes.add(Dense(10, activation='softmax'))\n",
        "    # Compile model\n",
        "nodes.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "nodes.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ...significant increase in accuracy from flat/vanilla node numbers initially used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning ReLU subparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n2500/2500 [==============================] - 68s 27ms/step - loss: 0.8806 - accuracy: 0.7837 - val_loss: 0.5557 - val_accuracy: 0.8304\nEpoch 2/5\n2500/2500 [==============================] - 65s 26ms/step - loss: 0.5294 - accuracy: 0.8423 - val_loss: 0.5829 - val_accuracy: 0.8299\nEpoch 3/5\n2500/2500 [==============================] - 58s 23ms/step - loss: 0.4825 - accuracy: 0.8590 - val_loss: 0.5220 - val_accuracy: 0.8485\nEpoch 4/5\n2500/2500 [==============================] - 58s 23ms/step - loss: 0.5499 - accuracy: 0.8624 - val_loss: 0.4805 - val_accuracy: 0.8607\nEpoch 5/5\n2500/2500 [==============================] - 58s 23ms/step - loss: 0.4230 - accuracy: 0.8809 - val_loss: 0.4868 - val_accuracy: 0.8618\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f415304be48>"
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.layers import ReLU\n",
        "# trying \n",
        "ReLU = Sequential([\n",
        "Dense(784, input_dim=784, activation='relu'),\n",
        "ReLU(negative_slope=0.1),\n",
        "Dense(512),\n",
        "ReLU(negative_slope=0.1),\n",
        "Dense(256),\n",
        "ReLU(negative_slope=0.1),\n",
        "Dense(128),\n",
        "ReLU(negative_slope=0.1),\n",
        "Dense(64),\n",
        "ReLU(negative_slope=0.1),\n",
        "Dense(10, activation='softmax')])\n",
        "    # Compile model\n",
        "ReLU.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "ReLU.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test)) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n2500/2500 [==============================] - 69s 27ms/step - loss: 1.1541 - accuracy: 0.7716 - val_loss: 0.6386 - val_accuracy: 0.8101\nEpoch 2/5\n2500/2500 [==============================] - 59s 24ms/step - loss: 0.6073 - accuracy: 0.8209 - val_loss: 0.6524 - val_accuracy: 0.8127\nEpoch 3/5\n2500/2500 [==============================] - 61s 24ms/step - loss: 0.5747 - accuracy: 0.8350 - val_loss: 0.6569 - val_accuracy: 0.8145\nEpoch 4/5\n2500/2500 [==============================] - 77s 31ms/step - loss: 0.5388 - accuracy: 0.8486 - val_loss: 0.5951 - val_accuracy: 0.8350\nEpoch 5/5\n2500/2500 [==============================] - 76s 30ms/step - loss: 0.4737 - accuracy: 0.8665 - val_loss: 0.5470 - val_accuracy: 0.8508\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f415029a710>"
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.layers import ReLU\n",
        "ReLU_thirty = Sequential([\n",
        "    Dense(784, input_dim=784, activation='relu'),\n",
        "    ReLU(negative_slope=0.3),\n",
        "    Dense(512),\n",
        "    ReLU(negative_slope=0.3),\n",
        "    Dense(256),\n",
        "    ReLU(negative_slope=0.3),\n",
        "    Dense(128),\n",
        "    ReLU(negative_slope=0.3),\n",
        "    Dense(64),\n",
        "    ReLU(negative_slope=0.3),\n",
        "    Dense(10, activation='softmax')])\n",
        "    # Compile model\n",
        "ReLU_thirty.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "ReLU_thirty.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning optimizer..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "import os\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([512]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'rmsprop']))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparams_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "      hparams=[HP_NUM_UNITS, HP_LEARNING_RATE, HP_OPTIMIZER],\n",
        "      metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_model(hparams):\n",
        "  model = tf.keras.Sequential(\n",
        "    [tf.keras.layers.Dense(hparams[HP_NUM_UNITS], input_dim=784, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')]\n",
        "  )\n",
        "  opt_name = hparams[HP_OPTIMIZER]\n",
        "  lr = hparams[HP_LEARNING_RATE]\n",
        "\n",
        "  if opt_name == 'adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  elif opt_name == 'nadam':\n",
        "    opt = tf.keras.optimizers.Nadam(learning_rate=lr)\n",
        "  elif opt_name == 'rmsprop':\n",
        "    opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "  else:\n",
        "    raise ValueError(f'Unexpected optimizer: {opt_name}')\n",
        "  \n",
        "  model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "  \n",
        "  model.fit(X_train, y_train, epochs=5)\n",
        "  _, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run(run_dir, hparams):\n",
        "    with tf.summary.create_file_writer(run_dir).as_default():\n",
        "        hp.hparams(hparams) # record the values used in this trial\n",
        "        accuracy = train_test_model(hparams)\n",
        "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--- Starting trial: run-0\n{'num_units': 512, 'learning_rate': 0.001, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 33s 13ms/step - loss: 1.0760 - accuracy: 0.7500\nEpoch 2/5\n2500/2500 [==============================] - 29s 12ms/step - loss: 0.5605 - accuracy: 0.8311\nEpoch 3/5\n2500/2500 [==============================] - 29s 12ms/step - loss: 0.4935 - accuracy: 0.8524\nEpoch 4/5\n2500/2500 [==============================] - 33s 13ms/step - loss: 0.4433 - accuracy: 0.8676\nEpoch 5/5\n2500/2500 [==============================] - 33s 13ms/step - loss: 0.3988 - accuracy: 0.8809\n625/625 [==============================] - 3s 5ms/step - loss: 0.4741 - accuracy: 0.8624\n--- Starting trial: run-1\n{'num_units': 512, 'learning_rate': 0.001, 'optimizer': 'rmsprop'}\nEpoch 1/5\n2500/2500 [==============================] - 52s 21ms/step - loss: 2.2057 - accuracy: 0.4253\nEpoch 2/5\n2500/2500 [==============================] - 49s 19ms/step - loss: 2.1460 - accuracy: 0.3302\nEpoch 3/5\n2500/2500 [==============================] - 48s 19ms/step - loss: 2.2744 - accuracy: 0.2659\nEpoch 4/5\n2500/2500 [==============================] - 35s 14ms/step - loss: 2.3134 - accuracy: 0.2100\nEpoch 5/5\n2500/2500 [==============================] - 33s 13ms/step - loss: 2.3171 - accuracy: 0.2359\n625/625 [==============================] - 3s 4ms/step - loss: 2.6602 - accuracy: 0.2704\n"
        }
      ],
      "source": [
        "session_num = 0\n",
        "\n",
        "# basically a grid search\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "    for learning_rate in (\n",
        "        #HP_LEARNING_RATE.domain.min_value,\n",
        "        HP_LEARNING_RATE.domain.values):\n",
        "        for optimizer in HP_OPTIMIZER.domain.values:\n",
        "            hparams = {\n",
        "                HP_NUM_UNITS: num_units,\n",
        "                HP_LEARNING_RATE: learning_rate,\n",
        "                HP_OPTIMIZER: optimizer\n",
        "            }\n",
        "\n",
        "            run_name = f'run-{session_num}'\n",
        "            print(f'--- Starting trial: {run_name}')\n",
        "            print({param.name:hparams[param] for param in hparams})\n",
        "            run('logs/hparams_tuning/' + run_name, hparams)\n",
        "            session_num += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ...adam reigns supreme!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning Learning Rate..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([512]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([.0001, .0005, .001, .005]))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparams_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "      hparams=[HP_NUM_UNITS, HP_LEARNING_RATE, HP_OPTIMIZER],\n",
        "      metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--- Starting trial: run-0\n{'num_units': 512, 'learning_rate': 0.0001, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 23s 9ms/step - loss: 2.6826 - accuracy: 0.6467\nEpoch 2/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 0.7504 - accuracy: 0.7832\nEpoch 3/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 0.5717 - accuracy: 0.8274\nEpoch 4/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 0.4626 - accuracy: 0.8574\nEpoch 5/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 0.3811 - accuracy: 0.8821\n625/625 [==============================] - 2s 3ms/step - loss: 0.5861 - accuracy: 0.8365\n--- Starting trial: run-1\n{'num_units': 512, 'learning_rate': 0.0005, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 1.2859 - accuracy: 0.7346\nEpoch 2/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 0.5619 - accuracy: 0.8307\nEpoch 3/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 0.4801 - accuracy: 0.8543\nEpoch 4/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 0.4168 - accuracy: 0.8747\nEpoch 5/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 0.3654 - accuracy: 0.8893\n625/625 [==============================] - 2s 4ms/step - loss: 0.4872 - accuracy: 0.8596\n--- Starting trial: run-2\n{'num_units': 512, 'learning_rate': 0.001, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 20s 8ms/step - loss: 1.1389 - accuracy: 0.7143\nEpoch 2/5\n2500/2500 [==============================] - 21s 8ms/step - loss: 0.5657 - accuracy: 0.8299\nEpoch 3/5\n2500/2500 [==============================] - 23s 9ms/step - loss: 0.4923 - accuracy: 0.8536\nEpoch 4/5\n2500/2500 [==============================] - 23s 9ms/step - loss: 0.4397 - accuracy: 0.8696\nEpoch 5/5\n2500/2500 [==============================] - 21s 8ms/step - loss: 0.3970 - accuracy: 0.8837\n625/625 [==============================] - 2s 4ms/step - loss: 0.4783 - accuracy: 0.8619\n--- Starting trial: run-3\n{'num_units': 512, 'learning_rate': 0.005, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 32s 13ms/step - loss: 1.3460 - accuracy: 0.7423\nEpoch 2/5\n2500/2500 [==============================] - 32s 13ms/step - loss: 0.6675 - accuracy: 0.8047\nEpoch 3/5\n2500/2500 [==============================] - 29s 11ms/step - loss: 0.6950 - accuracy: 0.8038\nEpoch 4/5\n2500/2500 [==============================] - 29s 11ms/step - loss: 0.6793 - accuracy: 0.8089\nEpoch 5/5\n2500/2500 [==============================] - 29s 12ms/step - loss: 0.7245 - accuracy: 0.7925\n625/625 [==============================] - 3s 5ms/step - loss: 0.7870 - accuracy: 0.8017\n"
        }
      ],
      "source": [
        "session_num = 0\n",
        "\n",
        "# basically a grid search\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "    for learning_rate in (\n",
        "        #HP_LEARNING_RATE.domain.min_value,\n",
        "        HP_LEARNING_RATE.domain.values):\n",
        "        for optimizer in HP_OPTIMIZER.domain.values:\n",
        "            hparams = {\n",
        "                HP_NUM_UNITS: num_units,\n",
        "                HP_LEARNING_RATE: learning_rate,\n",
        "                HP_OPTIMIZER: optimizer\n",
        "            }\n",
        "\n",
        "            run_name = f'run-{session_num}'\n",
        "            print(f'--- Starting trial: {run_name}')\n",
        "            print({param.name:hparams[param] for param in hparams})\n",
        "            run('logs/hparams_tuning/' + run_name, hparams)\n",
        "            session_num += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_model(hparams):\n",
        "  model = tf.keras.Sequential(\n",
        "    [tf.keras.layers.Dense(hparams[HP_NUM_UNITS], input_dim=784, activation='selu'),\n",
        "    tf.keras.layers.Dense(256, activation='selu'),\n",
        "    tf.keras.layers.Dense(128, activation='selu'),\n",
        "    tf.keras.layers.Dense(64, activation='selu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')]\n",
        "  )\n",
        "  opt_name = hparams[HP_OPTIMIZER]\n",
        "  lr = hparams[HP_LEARNING_RATE]\n",
        "\n",
        "  if opt_name == 'adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  else:\n",
        "    raise ValueError(f'Unexpected optimizer: {opt_name}')\n",
        "  \n",
        "  model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "  \n",
        "  model.fit(X_train, y_train, epochs=5)\n",
        "  _, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--- Starting trial: run-0\n{'num_units': 512, 'learning_rate': 0.001, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 51s 20ms/step - loss: 1.2538 - accuracy: 0.7496\nEpoch 2/5\n2500/2500 [==============================] - 52s 21ms/step - loss: 0.5676 - accuracy: 0.8283\nEpoch 3/5\n2500/2500 [==============================] - 51s 20ms/step - loss: 0.4940 - accuracy: 0.8506\nEpoch 4/5\n2500/2500 [==============================] - 52s 21ms/step - loss: 0.4394 - accuracy: 0.8680\nEpoch 5/5\n2500/2500 [==============================] - 52s 21ms/step - loss: 0.4009 - accuracy: 0.8816\n625/625 [==============================] - 6s 10ms/step - loss: 0.5411 - accuracy: 0.8487\n"
        }
      ],
      "source": [
        "session_num = 0\n",
        "\n",
        "# basically a grid search\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "    for learning_rate in [.001]:\n",
        "        for optimizer in HP_OPTIMIZER.domain.values:\n",
        "            hparams = {\n",
        "                HP_NUM_UNITS: num_units,\n",
        "                HP_LEARNING_RATE: learning_rate,\n",
        "                HP_OPTIMIZER: optimizer\n",
        "            }\n",
        "\n",
        "            run_name = f'run-{session_num}'\n",
        "            print(f'--- Starting trial: {run_name}')\n",
        "            print({param.name:hparams[param] for param in hparams})\n",
        "            run('logs/hparams_tuning/' + run_name, hparams)\n",
        "            session_num += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ...looks like relu > selu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--- Starting trial: run-0\n{'num_units': 32, 'learning_rate': 0.001, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 30s 12ms/step - loss: 1.5070 - accuracy: 0.6503\nEpoch 2/5\n2500/2500 [==============================] - 28s 11ms/step - loss: 0.6695 - accuracy: 0.7976\nEpoch 3/5\n2500/2500 [==============================] - 27s 11ms/step - loss: 0.5699 - accuracy: 0.8303\nEpoch 4/5\n2500/2500 [==============================] - 30s 12ms/step - loss: 0.5003 - accuracy: 0.8521\nEpoch 5/5\n2500/2500 [==============================] - 33s 13ms/step - loss: 0.4538 - accuracy: 0.8673\n625/625 [==============================] - 5s 8ms/step - loss: 0.5282 - accuracy: 0.8501\n--- Starting trial: run-1\n{'num_units': 64, 'learning_rate': 0.001, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 31s 13ms/step - loss: 1.5324 - accuracy: 0.6969\nEpoch 2/5\n2500/2500 [==============================] - 30s 12ms/step - loss: 0.6327 - accuracy: 0.8112\nEpoch 3/5\n2500/2500 [==============================] - 33s 13ms/step - loss: 0.5423 - accuracy: 0.8389\nEpoch 4/5\n2500/2500 [==============================] - 33s 13ms/step - loss: 0.4870 - accuracy: 0.8546\nEpoch 5/5\n2500/2500 [==============================] - 34s 14ms/step - loss: 0.4547 - accuracy: 0.8659\n625/625 [==============================] - 5s 9ms/step - loss: 0.5442 - accuracy: 0.8472\n--- Starting trial: run-2\n{'num_units': 128, 'learning_rate': 0.001, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 36s 15ms/step - loss: 1.6098 - accuracy: 0.6424\nEpoch 2/5\n2500/2500 [==============================] - 35s 14ms/step - loss: 0.6666 - accuracy: 0.7942\nEpoch 3/5\n2500/2500 [==============================] - 36s 14ms/step - loss: 0.5535 - accuracy: 0.8328\nEpoch 4/5\n2500/2500 [==============================] - 35s 14ms/step - loss: 0.4945 - accuracy: 0.8535\nEpoch 5/5\n2500/2500 [==============================] - 34s 14ms/step - loss: 0.4542 - accuracy: 0.8662\n625/625 [==============================] - 5s 8ms/step - loss: 0.5243 - accuracy: 0.8493\n--- Starting trial: run-3\n{'num_units': 256, 'learning_rate': 0.001, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 34s 14ms/step - loss: 1.6920 - accuracy: 0.7031\nEpoch 2/5\n2500/2500 [==============================] - 35s 14ms/step - loss: 0.6391 - accuracy: 0.8098\nEpoch 3/5\n2500/2500 [==============================] - 35s 14ms/step - loss: 0.5453 - accuracy: 0.8375\nEpoch 4/5\n2500/2500 [==============================] - 34s 14ms/step - loss: 0.4933 - accuracy: 0.8537\nEpoch 5/5\n2500/2500 [==============================] - 35s 14ms/step - loss: 0.4520 - accuracy: 0.8665\n625/625 [==============================] - 5s 8ms/step - loss: 0.5693 - accuracy: 0.8446\n--- Starting trial: run-4\n{'num_units': 512, 'learning_rate': 0.001, 'optimizer': 'adam'}\nEpoch 1/5\n2500/2500 [==============================] - 33s 13ms/step - loss: 1.7294 - accuracy: 0.7174\nEpoch 2/5\n2500/2500 [==============================] - 36s 14ms/step - loss: 0.6233 - accuracy: 0.8115\nEpoch 3/5\n2500/2500 [==============================] - 37s 15ms/step - loss: 0.5478 - accuracy: 0.8381\nEpoch 4/5\n2500/2500 [==============================] - 35s 14ms/step - loss: 0.4947 - accuracy: 0.8535\nEpoch 5/5\n2500/2500 [==============================] - 35s 14ms/step - loss: 0.4447 - accuracy: 0.8678\n625/625 [==============================] - 5s 8ms/step - loss: 0.5099 - accuracy: 0.8540\n--- Starting trial: run-5\n{'num_units': 784, 'learning_rate': 0.001, 'optimizer': 'adam'}\nEpoch 1/5\n   1/2500 [..............................] - ETA: 0s - loss: 115.0984 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0121s). Check your callbacks.\nWARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0121s). Check your callbacks.\n2500/2500 [==============================] - 36s 14ms/step - loss: 1.5190 - accuracy: 0.6139\nEpoch 2/5\n2500/2500 [==============================] - 37s 15ms/step - loss: 0.6724 - accuracy: 0.7987\nEpoch 3/5\n2500/2500 [==============================] - 38s 15ms/step - loss: 0.5540 - accuracy: 0.8366\nEpoch 4/5\n2500/2500 [==============================] - 38s 15ms/step - loss: 0.4944 - accuracy: 0.8547\nEpoch 5/5\n2500/2500 [==============================] - 36s 15ms/step - loss: 0.4477 - accuracy: 0.8701\n625/625 [==============================] - 5s 9ms/step - loss: 0.5349 - accuracy: 0.8464\n"
        }
      ],
      "source": [
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([784, 512, 256, 128, 64, 32]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([.001]))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparams_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "      hparams=[HP_NUM_UNITS, HP_LEARNING_RATE, HP_OPTIMIZER],\n",
        "      metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]\n",
        "  )\n",
        "\n",
        "def train_test_model(hparams):\n",
        "  model = tf.keras.Sequential(\n",
        "    [tf.keras.layers.Dense(256, input_dim=784, activation='selu'),\n",
        "    tf.keras.layers.Dense(128, activation='selu'),\n",
        "    tf.keras.layers.Dense(64, activation='selu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')]\n",
        "  )\n",
        "  opt_name = hparams[HP_OPTIMIZER]\n",
        "  lr = hparams[HP_LEARNING_RATE]\n",
        "\n",
        "  if opt_name == 'adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  else:\n",
        "    raise ValueError(f'Unexpected optimizer: {opt_name}')\n",
        "  \n",
        "  model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "  \n",
        "  model.fit(X_train, y_train, epochs=5)\n",
        "  _, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "session_num = 0\n",
        "\n",
        "# basically a grid search\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "    for learning_rate in HP_LEARNING_RATE.domain.values:\n",
        "        for optimizer in HP_OPTIMIZER.domain.values:\n",
        "            hparams = {\n",
        "                HP_NUM_UNITS: num_units,\n",
        "                HP_LEARNING_RATE: learning_rate,\n",
        "                HP_OPTIMIZER: optimizer\n",
        "            }\n",
        "\n",
        "            run_name = f'run-{session_num}'\n",
        "            print(f'--- Starting trial: {run_name}')\n",
        "            print({param.name:hparams[param] for param in hparams})\n",
        "            run('logs/hparams_tuning/' + run_name, hparams)\n",
        "            session_num += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 512 first layer wins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "ERROR: Failed to launch TensorBoard (exited with 1).\nContents of stderr:\n2020-09-10 12:19:27.503717: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n2020-09-10 12:19:27.503765: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTraceback (most recent call last):\n  File \"/home/c/anaconda3/envs/nn/bin/tensorboard\", line 11, in <module>\n    sys.exit(run_main())\n  File \"/home/c/anaconda3/envs/nn/lib/python3.7/site-packages/tensorboard/main.py\", line 59, in run_main\n    program.get_default_assets_zip_provider())\n  File \"/home/c/anaconda3/envs/nn/lib/python3.7/site-packages/tensorboard/program.py\", line 144, in __init__\n    self.plugin_loaders = [make_loader(p) for p in plugins]\n  File \"/home/c/anaconda3/envs/nn/lib/python3.7/site-packages/tensorboard/program.py\", line 144, in <listcomp>\n    self.plugin_loaders = [make_loader(p) for p in plugins]\n  File \"/home/c/anaconda3/envs/nn/lib/python3.7/site-packages/tensorboard/program.py\", line 143, in make_loader\n    raise ValueError(\"Not a TBLoader or TBPlugin subclass: %s\" % plugin)\nValueError: Not a TBLoader or TBPlugin subclass: <class 'tensorboard_plugin_wit.wit_plugin_loader.WhatIfToolPluginLoader'>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs/hparam_tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning subparameter within Relu activation function\n",
        "### return to this to debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded while calling a Python object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-81182a245745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Compile model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m       self.compiled_loss = compile_utils.LossesContainer(\n\u001b[1;32m    543\u001b[0m           loss, loss_weights, output_names=self.output_names)\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    338\u001b[0m                          ' Always start with this line.'), None)\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2770\u001b[0m     \u001b[0;31m# Keep track of trackable objects, for the needs of `Network.save_weights`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2771\u001b[0m     value = data_structures.sticky_attribute_assignment(\n\u001b[0;32m-> 2772\u001b[0;31m         trackable=self, value=value, name=name)\n\u001b[0m\u001b[1;32m   2773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m     \u001b[0mreference_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj_reference_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36msticky_attribute_assignment\u001b[0;34m(trackable, name, value)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# name, since assigning a new variable to an attribute has\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# historically been fine (e.g. Adam did this).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         overwrite=True)\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_track_trackable\u001b[0;34m(self, trackable, name, overwrite)\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0mnew_reference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrackableReference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mcurrent_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lookup_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_object\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcurrent_object\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_lookup_dependency\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_lookup_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mlayer_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer_checkpoint_dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_dependencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_layer_checkpoint_dependencies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m           \u001b[0;31m# Keep a separate index for layers which have weights. This allows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m           \u001b[0;31m# users to insert Layers without weights anywhere in the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \"\"\"\n\u001b[0;32m-> 1354\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \"\"\"\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m       \u001b[0mchildren_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchildren_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   2842\u001b[0m       return list(\n\u001b[1;32m   2843\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2844\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2842\u001b[0m       return list(\n\u001b[1;32m   2843\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 2844\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   2845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1895\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m             \u001b[0msub_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1897\u001b[0;31m             extra_variables=self._trainable_weights))\n\u001b[0m\u001b[1;32m   1898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mgather_trainable_weights\u001b[0;34m(trainable, sub_layers, extra_variables)\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msub_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m   trainable_extra_variables = [\n\u001b[1;32m    273\u001b[0m       v for v in extra_variables if v.trainable]\n",
            "... last 5 frames repeated, from the frame below ...\n",
            "\u001b[0;32m~/anaconda3/envs/nn/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \"\"\"\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m       \u001b[0mchildren_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchildren_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
          ]
        }
      ],
      "source": [
        "\n",
        "relu = Sequential()\n",
        "relu.add(Dense(784, input_dim=784, activation=relu))\n",
        "#relu.add(tf.keras.layers.ReLU(threshold=0.2))\n",
        "relu.add(Dense(512, activation='selu'))\n",
        "relu.add(Dense(256, activation='selu'))\n",
        "relu.add(Dense(128, activation='selu'))\n",
        "relu.add(Dense(64, activation='selu'))\n",
        "relu.add(Dense(10, activation='softmax'))\n",
        "    # Compile model\n",
        "relu.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "relu.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DONE Optimizer\n",
        "DONE Learning Rate\n",
        "DONE Activiation Function\n",
        "DONE At least 1 subparameter within the Relu activation function\n",
        "DONE Number of Neurons in Hidden Layers\n",
        "DONE Number of Hidden Layers\n",
        "____ Weight Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKbr1gRg9BXs",
        "colab_type": "text"
      },
      "source": [
        "### Stretch Goals\n",
        "- Implement Bayesian Hyper-parameter Optimization\n",
        "- Select a new dataset and apply a neural network to it.\n",
        "- Use a cloud base experiment tracking framework such as weights and biases\n",
        "- Research potential architecture ideas for this problem. Try Lenet-10 for example. "
      ]
    }
  ]
}